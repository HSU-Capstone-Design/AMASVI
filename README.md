# AMASVI
A Multimodal Assistance System for the Visually Impaired


## 작품개요

본 프로젝트는 객체 탐지(YOLO), 깊이 추정(GLPDepth), 광학 문자 판독(OCR), 그리고 대규모 언어 모델(LLM) 기술을 융합하여 개발한 시각장애인 보행 보조 시스템이다. 카메라를 통해 실시간으로 입력된 영상 데이터를 처리하여 주변 환경의 객체와 거리를 인식하고, 보행 경로상 잠재적인 위험 요소를 즉각 탐지하여 사용자에게 경고한다. 또한, OCR을 통해 주변의 문자 정보를 추출한 뒤, LLM을 활용해 의미 있는 정보를 분석하고 이를 음성으로 제공함으로써 사용자 이해를 돕는다. 아울러, 버튼 인터럽트 기반의 음성 명령 기능을 통해 사용자가 능동적으로 정보를 요청할 수 있도록 지원하여 보다 안전하고 독립적인 보행을 가능하게 한다.

## 주요 기능

- **YOLO 객체 탐지**: 실시간으로 화면 내 주요 장애물(차량, 전봇대 등)을 인식  
- **ML Kit OCR**: 화면에 보이는 텍스트(간판, 표지판 등)를 실시간으로 인식  
- **GLPDepth**: 카메라 영상으로부터 심도(depth) 맵을 생성해 거리 측정  
- **STT**: 볼륨 다운 버튼 롱프레스 → 음성 인식 시작 → 키워드 추출  
- **LLM(OpenAI GPT-3.5-turbo) 연동**  
  - 음성 인식 결과에서 단일 키워드 추출  
  - OCR 결과 리스트 중 키워드 매칭  
- **TTS**: 위험 구역, 키워드 관련 지점 탐지 시 거리 및 방향 음성 안내  

## 구조

## 시스템 워크플로우
앱 실행 → 항상 장애물 감지 시 경고 음성 안내

On Demand → 볼륨 ↓ 버튼 길게 누름 → 음성인식 시작

ex) “주변에 카페 있으면 알려줘” 발화 → LLM을 통해 키워드("카페") 추출

화면 내 텍스트 OCR 인식 → “카페” 매칭 키워드 찾기

찾은 지점 거리·방향 음성 안내

## 작동화면 예시

## 기대효과
시각장애인의 독립적이고 안전한 보행을 실질적으로 지원하여, 이동 중 발생할 수 있는 위험을 예방하고 일상생활에서의 자율성과 활동성을 크게 향상시킬 것으로 기대된다. 또한 객체 탐지, 거리 측정, 문자 인식을 통해 주변 환경에 대한 직관적인 이해를 돕고, 사용자 맞춤형 음성 정보 제공으로 삶의 질 향상에 기여할 것이다. 향후 GPS 및 실내 위치 인식 기술과의 연계를 통해 정밀한 내외부 내비게이션 기능을 제공할 수 있으며, 긴급 상황 발생 시 주변에 도움을 요청할 수 있는 기능으로도 확장 가능하다.



